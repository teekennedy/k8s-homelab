# https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
kube-prometheus-stack:
  ## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.
  ##
  cleanPrometheusOperatorObjectNames: true
  ## Grafana subchart values: https://github.com/grafana-community/helm-charts/blob/main/charts/grafana/values.yaml
  grafana:
    admin:
      existingSecret: monitoring-system-grafana
      userKey: username
      passwordKey: password
    enabled: true
    defaultDashboardsEnabled: false
    deploymentStrategy:
      type: Recreate
    serviceMonitor:
      labels:
        release: monitoring-system
    ingress:
      enabled: true
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod
        gethomepage.dev/enabled: "true"
        gethomepage.dev/group: "Cluster"
        gethomepage.dev/name: "Grafana"
        gethomepage.dev/description: "Monitoring and dashboards"
        gethomepage.dev/icon: "sh-grafana.svg"
        gethomepage.dev/pod-selector: "app.kubernetes.io/name=grafana"
      hosts:
        - &host grafana.msng.to
      tls:
        - secretName: grafana-general-tls
          hosts:
            - *host
    persistence:
      storageClassName: longhorn
      accessModes:
        - ReadWriteOnce
      enabled: true
      size: 10Gi
      type: pvc
    sidecar:
      dashboards:
        enabled: true
      datasources:
        enabled: true
    additionalDataSources:
      - name: Loki
        type: loki
        access: proxy
        url: http://monitoring-system-loki:3100
        isDefault: false
        editable: false
        jsonData:
          timeInterval: 30s
      - name: VictoriaMetrics
        type: prometheus
        access: proxy
        url: http://victoria-metrics-server.victoria-metrics.svc:8428
        isDefault: false
        editable: false
        jsonData:
          timeInterval: 30s
    # This used to contain Dex client secret, but currently doesn't exist.
    # Can be recreated if needed with GRAFANA_SSO_CLIENT_SECRET: (dex.grafana -> client_secret in global-secrets namespace)
    # envFromSecret: grafana-secrets
    grafana.ini:
      server:
        root_url: https://grafana.msng.to
      # TODO integrate with Authelia
      auth.generic_oauth:
        enabled: false
        # Allows OAuth users without existing grafana accounts to login
        # allow_sign_up: true
        # Bypass login selection screen
        # auto_login: true
        name: SSO
        client_id: grafana-sso
        client_secret: $__env{GRAFANA_SSO_CLIENT_SECRET}
        scopes: openid profile email groups offline_access
        auth_url: https://dex.msng.to/auth
        token_url: https://dex.msng.to/token
        api_url: https://dex.msng.to/userinfo

  homepage-grafana:
    enabled: true
    user:
      name: homepage
      email: homepage@msng.to
    secret:
      name: homepage-grafana-credentials

  defaultRules:
    create: false
  ## Configuration for alertmanager
  alertmanager:
    enabled: true

    alertmanagerSpec:
      secrets:
        - alertmanager-secrets
      configMaps:
        - alert-templates

    config:
      templates:
        - /etc/alertmanager/configmaps/alert-templates/*.tmpl
      global:
        resolve_timeout: 5m
      route:
        receiver: discord
        group_by: ['alertname', 'severity', 'namespace']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        routes:
          - receiver: "null"
            matchers:
              - alertname = "Watchdog"

      receivers:
        - name: "null"
        - name: discord
          discord_configs:
            - send_resolved: true
              title: '{{ template "__discord_subject" . }}'
              message: '{{ template "__discord_alert_list" . }}'
              webhook_url_file: /etc/alertmanager/secrets/alertmanager-secrets/discord_webhook_url

  ## Install Prometheus Operator CRDs
  ##
  crds:
    enabled: true
    ## The CRD upgrade job mitigates the limitation of helm not being able to upgrade CRDs.
    ## The job will apply the CRDs to the cluster before the operator is deployed, using helm hooks.
    ## It deploy a corresponding clusterrole, clusterrolebinding and serviceaccount to apply the CRDs.
    ## This feature is in preview, off by default and may change in the future.
    upgradeJob:
      enabled: true
      forceConflicts: true

  ##
  global:
    rbac:
      create: true

      ## Create ClusterRoles that extend the existing view, edit and admin ClusterRoles to interact with prometheus-operator CRDs
      ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles
      createAggregateClusterRoles: true

  # k3s bundles kubelet, apiserver controllermanager, scheduler, and etcd into a single binary.
  # Scrape kubelet for all metrics
  kubelet:
    enabled: true
    serviceMonitor:
      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      metricRelabelings:
        # Drop excessively noisy apiserver and etcd buckets.
        # These 7 metrics accounted for over 43% of my total series count before their removal.
        - action: drop
          regex: (apiserver|etcd)_(request_duration_seconds|request_sli_duration_seconds|request_body_size_bytes|response_sizes|watch_cache_read_wait_seconds|watch_events_sizes)_bucket
          sourceLabels:
            - __name__
  kubeApiServer: {}
  kubeControllerManager:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeScheduler:
    enabled: false
  kubeProxy:
    enabled: false

  ## Manages Prometheus and Alertmanager components
  ##
  prometheusOperator:
    # Running off of my fork of prometheus-operator to test discord webhookURLFile fix
    image:
      registry: ghcr.io
      repository: teekennedy/prometheus-operator
      tag: discord-webhook-file-fix@sha256:9214d2f42956c8a6f95c8c662389fd4d3933ba88b2a5cb5b847321dc9138c076
      pullPolicy: IfNotPresent
    ## Create Endpoints objects for kubelet targets.
    kubeletEndpointsEnabled: false
    ## Create EndpointSlice objects for kubelet targets.
    kubeletEndpointSliceEnabled: true

    ## Operator Environment
    ##  env:
    ##    VARIABLE: value
    env:
      GOGC: "30"
      TZ: America/Denver

    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault

    ## Container-specific security context configuration
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    ##
    containerSecurityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      capabilities:
        drop:
          - ALL

  prometheus:
    ingress:
      enabled: true
      pathType: Prefix
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod
        gethomepage.dev/enabled: "true"
        gethomepage.dev/group: "Cluster"
        gethomepage.dev/name: "Prometheus"
        gethomepage.dev/description: "Metrics and alerting"
        gethomepage.dev/icon: "sh-prometheus.svg"
        gethomepage.dev/pod-selector: "app.kubernetes.io/name=prometheus"
        gethomepage.dev/widget.type: "prometheus"
        gethomepage.dev/widget.url: "https://prometheus.msng.to"
      hosts:
        - &promHost prometheus.msng.to
      tls:
        - secretName: prometheus-tls
          hosts:
            - *promHost
    ## Settings affecting prometheusSpec
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#prometheusspec
    ##
    prometheusSpec:
      externalUrl: https://prometheus.msng.to

      resources:
        requests:
          cpu: 500m
          memory: 2Gi
        limits:
          cpu: 2000m
          memory: 4Gi

      # By default, if you don't specify a selector for prometheus CRDs, it will write a selector that matches only the CRDs that are defined in this helm release
      # Disabling this to pick up all relevant CRDs
      # https://stackoverflow.com/q/68085831
      podMonitorSelectorNilUsesHelmValues: false
      probeSelectorNilUsesHelmValues: false
      ruleSelectorNilUsesHelmValues: false
      serviceMonitorSelectorNilUsesHelmValues: false
      scrapeConfigSelectorNilUsesHelmValues: false

      # Remote write to VictoriaMetrics for long-term storage
      remoteWrite:
        - url: "http://victoria-metrics-server.victoria-metrics.svc:8428/api/v1/write"

      # Prometheus docs say to set this to maximum 80-85% of the volume size
      # https://prometheus.io/docs/prometheus/latest/storage/#operational-aspects
      retention: "10d"
      retentionSize: "160GiB"
      # Set retentionTime to empty value so prometheus relies solely on retentionSize
      retrentionTime: ""

      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: longhorn
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 220Gi

loki:
  enabled: true
  deploymentMode: SingleBinary
  loki:
    auth_enabled: false
    commonConfig:
      replication_factor: 1
    # Pattern ingester adds CPU/memory overhead; disable if it is too expensive.
    pattern_ingester:
      enabled: true
    storage:
      type: filesystem
    schemaConfig:
      configs:
        - from: 2026-01-01
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            prefix: loki_index_
            period: 24h
  singleBinary:
    replicas: 1
    persistence:
      enabled: true
      size: 50Gi
      storageClass: longhorn
  read:
    replicas: 0
  write:
    replicas: 0
  backend:
    replicas: 0

promtail:
  enabled: true
  config:
    clients:
      - url: http://monitoring-system-loki:3100/loki/api/v1/push
    snippets:
      extraScrapeConfigs: |
        - job_name: systemd-journal
          journal:
            path: /run/log/journal
            max_age: 24h
            labels:
              job: systemd-journal
          relabel_configs:
            - source_labels: ['__journal__systemd_unit']
              target_label: 'systemd_unit'
            - source_labels: ['__journal__hostname']
              target_label: 'host'
        - job_name: dmesg
          journal:
            path: /run/log/journal
            max_age: 24h
            matches: _TRANSPORT=kernel
            labels:
              job: dmesg
          relabel_configs:
            - source_labels: ['__journal__hostname']
              target_label: 'host'
  extraVolumes:
    - name: journal
      hostPath:
        path: /run/log/journal
        type: DirectoryOrCreate
    - name: machine-id
      hostPath:
        path: /etc/machine-id
        type: File
  extraVolumeMounts:
    - name: journal
      mountPath: /run/log/journal
      readOnly: true
    - name: machine-id
      mountPath: /etc/machine-id
      readOnly: true
